{
  "openapi": "3.0.1",
  "x-stoplight": {
    "id": "ob6ig5qv6o57h"
  },
  "info": {
    "title": "Lucidworks AI Async Prediction API",
    "version": "v0",
    "description": "The Lucidworks AI Async Prediction API contains two requests:\n\n* POST Request: This request is used to submit a prediction task for a specific `useCase` and `modelId`. Upon submission, the API responds with the following information:\n\n    * `predictionId` that is a unique UUID for the submitted prediction task, and can be used later to retrieve the results. \n\n    * `status` that indicates the current state of the prediction task. \n \n* GET Request: This request is used to retrieve the results of a previously-submitted prediction request. You must provide the unique `predictionId` received from the POST request. The API then returns the results of the prediction request associated with that `predictionId`.\n\nLucidworks created and deployed the `mistral-7b-instruct` and `llama-3-8b-instruct` models. The Use Case API returns a list of all supported models.\n\nThe `async-prediction` endpoints require an authentication token with scope `machinelearning.predict`.",
    "contact": {
      "name": "Lucidworks",
      "url": "https://lucidworks.com/",
      "email": "support@lucidworks.com"
    },
    "termsOfService": "https://lucidworks.com/legal/developer-license-agreement/",
    "license": {
      "name": "Lucidworks",
      "url": "https://lucidworks.com/legal/developer-license-agreement/"
    }
  },
  "servers": [
    {
      "url": "https://APPLICATION_ID.applications.lucidworks.com",
      "description": "Production"
    }
  ],
  "paths": {
    "/ai/async-prediction/{USE_CASE}/{MODEL_ID}": {
      "post": {
        "summary": "Model predictions by use case",
        "description": "This is a basic example to submit a prediction task for a specific `useCase` and `modelId`. Upon submission, the API responds with a unique `predictionId` and a `status`. The `predictionId` can be used later in the GET request to retrieve the results.\n\nIMPORTANT: The available use cases are detailed in their own section in this specification.",
        "operationId": "post-ai-async-prediction-usecase-modelId",
        "parameters": [
          {
            "name": "USE_CASE",
            "in": "path",
            "description": "The name of the use case for the model.",
            "required": true,
            "schema": {
              "type": "string"
            }
          },
          {
            "schema": {
              "type": "string"
            },
            "in": "header",
            "name": "Authorization: Bearer ACCESS_TOKEN",
            "description": "The authentication and authorization access token."
          },
          {
            "schema": {
              "type": "string"
            },
            "in": "header",
            "name": "Content-Type",
            "description": "application/json"
          }
        ],
        "requestBody": {
          "content": {
            "application/json": {
              "schema": {
                "$ref": "#/components/schemas/BasicGenericRequest"
              },
              "examples": {}
            }
          },
          "required": true,
          "description": "Request information varies based on the use case in the request. See the specific use case for valid information for that use case."
        },
        "responses": {
          "200": {
            "description": "OK",
            "content": {
              "application/json": {
                "schema": {
                  "$ref": "#/components/schemas/POSTresponse"
                },
                "examples": {}
              }
            }
          },
          "4XX": {
            "$ref": "#/components/responses/Error"
          }
        },
        "x-stoplight": {
          "id": "z3dobtc99bwhw"
        }
      },
      "parameters": [
        {
          "schema": {
            "type": "string"
          },
          "name": "USE_CASE",
          "in": "path",
          "required": true,
          "description": "The name of the use case for the model."
        },
        {
          "schema": {
            "type": "string"
          },
          "name": "MODEL_ID",
          "in": "path",
          "required": true,
          "description": "The unique identifier for the model."
        }
      ]
    },
    "/ai/async-prediction/passthrough/{MODEL_ID}": {
      "post": {
        "summary": "Passthrough use case",
        "operationId": "post-ai-async-prediction-passthrough-modelId",
        "responses": {
          "200": {
            "description": "OK",
            "content": {
              "application/json": {
                "schema": {
                  "$ref": "#/components/schemas/POSTresponse"
                },
                "examples": {}
              }
            }
          },
          "4XX": {
            "$ref": "#/components/responses/Error"
          }
        },
        "description": "The passthrough use case lets you use the service as a proxy to the large language model (LLM). The service sends text (no additional prompts or other information) to the LLM. \n\nThe POST request obtains and indexes prediction information related to the specified use case, and returns a unique `predictionId` and `status` of the request. The `predictionId` can be used later in the GET request to retrieve the results.",
        "parameters": [
          {
            "schema": {
              "type": "string"
            },
            "in": "header",
            "name": "Authorization: Bearer ACCESS_TOKEN",
            "description": "The authentication and authorization access token."
          },
          {
            "schema": {
              "type": "string",
              "example": "application/json"
            },
            "in": "header",
            "name": "Content-Type",
            "description": "application/json"
          }
        ],
        "requestBody": {
          "content": {
            "application/json": {
              "schema": {
                "$ref": "#/components/schemas/PassthroughRequest"
              },
              "examples": {}
            }
          }
        },
        "x-stoplight": {
          "id": "ce2ejgsvrwy8o"
        }
      },
      "parameters": [
        {
          "schema": {
            "type": "string",
            "example": "6a092bd4-5098-466c-94aa-40bf6829430\""
          },
          "name": "MODEL_ID",
          "in": "path",
          "required": true,
          "description": "Unique identifier for the model."
        }
      ]
    },
    "/ai/async-prediction/rag/{MODEL_ID}": {
      "post": {
        "summary": "RAG use case",
        "operationId": "post-ai-prediction-rag-modelId-external-documents",
        "responses": {
          "200": {
            "description": "OK",
            "content": {
              "application/json": {
                "schema": {
                  "$ref": "#/components/schemas/POSTresponse"
                },
                "examples": {}
              }
            }
          },
          "4XX": {
            "$ref": "#/components/responses/Error"
          }
        },
        "description": "The rag use case uses candidate documents that are inserted into a LLM’s context to ground the generated response to those documents instead of generating an answer from details stored in the LLM’s trained weights. This type of search adds guardrails so the LLM can search private data collections.\n\nThe RAG search can perform queries against external documents passed in as part of the request.\n\nThe POST request obtains and indexes prediction information related to the specified use case, and returns a unique `predictionId` and `status` of the request. The `predictionId` can be used later in the GET request to retrieve the results.\n",
        "parameters": [
          {
            "schema": {
              "type": "string"
            },
            "in": "header",
            "name": "Authorization: Bearer ACCESS_TOKEN",
            "description": "The authentication and authorization access token."
          },
          {
            "schema": {
              "type": "string",
              "example": "application/json"
            },
            "in": "header",
            "name": "Content-Type",
            "description": "application/json"
          }
        ],
        "requestBody": {
          "content": {
            "application/json": {
              "schema": {
                "$ref": "#/components/schemas/RagExtDocRequest"
              },
              "examples": {}
            }
          }
        },
        "x-stoplight": {
          "id": "oqzil7g3yk237"
        }
      },
      "parameters": [
        {
          "schema": {
            "type": "string",
            "example": "6a092bd4-5098-466c-94aa-40bf6829430\""
          },
          "name": "MODEL_ID",
          "in": "path",
          "required": true,
          "description": "Unique identifier for the model."
        }
      ]
    },
    "/ai/async-prediction/standalone_query_rewriter/{MODEL_ID}": {
      "post": {
        "summary": "Standalone query rewriter",
        "operationId": "post-ai-prediction-standalone-query-rewriter-modelId",
        "responses": {
          "200": {
            "description": "OK",
            "content": {
              "application/json": {
                "schema": {
                  "$ref": "#/components/schemas/POSTresponse"
                },
                "examples": {}
              }
            }
          },
          "4XX": {
            "$ref": "#/components/responses/Error"
          }
        },
        "description": "The standalone query rewriter use case rewrites the text in reference to the context based on the memoryUuid.\n\nThe POST request obtains and indexes prediction information related to the specified use case, and returns a unique `predictionId` and `status` of the request. The `predictionId` can be used later in the GET request to retrieve the results.\n",
        "parameters": [
          {
            "schema": {
              "type": "string"
            },
            "in": "header",
            "name": "Authorization: Bearer ACCESS_TOKEN",
            "description": "The authentication and authorization access token."
          },
          {
            "schema": {
              "type": "string",
              "example": "application/json"
            },
            "in": "header",
            "name": "Content-Type",
            "description": "application/json"
          }
        ],
        "requestBody": {
          "content": {
            "application/json": {
              "schema": {
                "$ref": "#/components/schemas/StandaloneQueryRewriterRequest"
              },
              "examples": {}
            }
          }
        },
        "x-stoplight": {
          "id": "tl3w5vqma3ung"
        }
      },
      "parameters": [
        {
          "schema": {
            "type": "string",
            "example": "6a092bd4-5098-466c-94aa-40bf6829430\""
          },
          "name": "MODEL_ID",
          "in": "path",
          "required": true,
          "description": "Unique identifier for the model."
        }
      ]
    },
    "/ai/async-prediction/summarization/{MODEL_ID}": {
      "post": {
        "summary": "Summarization use case",
        "operationId": "post-ai-prediction-summarization-modelId",
        "responses": {
          "200": {
            "description": "OK",
            "content": {
              "application/json": {
                "schema": {
                  "$ref": "#/components/schemas/POSTresponse"
                },
                "examples": {}
              }
            }
          },
          "4XX": {
            "$ref": "#/components/responses/Error"
          }
        },
        "description": "In the summarization use case, the LLM ingests text and returns a summary of the text as a response.\n\nThe context length is 2048 tokens. No options can be configured.\n\nThe POST request obtains and indexes prediction information related to the specified use case, and returns a unique `predictionId` and `status` of the request. The `predictionId` can be used later in the GET request to retrieve the results.\n",
        "parameters": [
          {
            "schema": {
              "type": "string"
            },
            "in": "header",
            "name": "Authorization: Bearer ACCESS_TOKEN",
            "description": "The authentication and authorization access token."
          },
          {
            "schema": {
              "type": "string",
              "example": "application/json"
            },
            "in": "header",
            "name": "Content-Type",
            "description": "application/json"
          }
        ],
        "requestBody": {
          "content": {
            "application/json": {
              "schema": {
                "$ref": "#/components/schemas/SummarizationRequest"
              },
              "examples": {}
            }
          }
        },
        "x-stoplight": {
          "id": "iik43dur2pvus"
        }
      },
      "parameters": [
        {
          "schema": {
            "type": "string",
            "example": "6a092bd4-5098-466c-94aa-40bf6829430\""
          },
          "name": "MODEL_ID",
          "in": "path",
          "required": true,
          "description": "Unique identifier for the model."
        }
      ]
    },
    "/ai/async-prediction/keyword_extraction/{MODEL_ID}": {
      "post": {
        "summary": "Keyword extraction use case",
        "operationId": "post-ai-prediction-keyword-extraction-modelId",
        "responses": {
          "200": {
            "description": "OK",
            "content": {
              "application/json": {
                "schema": {
                  "$ref": "#/components/schemas/POSTresponse"
                },
                "examples": {}
              }
            }
          },
          "4XX": {
            "$ref": "#/components/responses/Error"
          }
        },
        "description": "In the keyword_extraction use case, the LLM ingests text and returns a JSON response that contains a list of keywords extracted from the text. No options can be configured.\n\nThe POST request obtains and indexes prediction information related to the specified use case, and returns a unique `predictionId` and `status` of the request. The `predictionId` can be used later in the GET request to retrieve the results.\n",
        "parameters": [
          {
            "schema": {
              "type": "string"
            },
            "in": "header",
            "name": "Authorization: Bearer ACCESS_TOKEN",
            "description": "The authentication and authorization access token."
          },
          {
            "schema": {
              "type": "string",
              "example": "application/json"
            },
            "in": "header",
            "name": "Content-Type",
            "description": "application/json"
          }
        ],
        "requestBody": {
          "content": {
            "application/json": {
              "schema": {
                "$ref": "#/components/schemas/KeywordExtractionRequest"
              },
              "examples": {}
            }
          }
        },
        "x-stoplight": {
          "id": "dslro5144lrh0"
        }
      },
      "parameters": [
        {
          "schema": {
            "type": "string",
            "example": "6a092bd4-5098-466c-94aa-40bf6829430\""
          },
          "name": "MODEL_ID",
          "in": "path",
          "required": true,
          "description": "Unique identifier for the model."
        }
      ]
    },
    "/ai/async-prediction/ner/{MODEL_ID}": {
      "post": {
        "summary": "Named Entity Recognition (NER) use case",
        "operationId": "post-ai-prediction-ner-modelId",
        "responses": {
          "200": {
            "description": "OK",
            "content": {
              "application/json": {
                "schema": {
                  "$ref": "#/components/schemas/POSTresponse"
                }
              }
            }
          },
          "4XX": {
            "$ref": "#/components/responses/Error"
          }
        },
        "description": "In the Named Entity Recognition (NER) use case, the LLM ingests text and entities to extract and return a JSON response that contains a list of entities extracted from the text. No options can be configured.\n\nThis use case can be used to extract nouns and proper nouns such as Brand, Date, Company, Places, and Category in order to guide and refine searches.\n\nThe POST request obtains and indexes prediction information related to the specified use case, and returns a unique `predictionId` and `status` of the request. The `predictionId` can be used later in the GET request to retrieve the results.\n",
        "parameters": [
          {
            "schema": {
              "type": "string"
            },
            "in": "header",
            "name": "Authorization: Bearer ACCESS_TOKEN",
            "description": "The authentication and authorization access token."
          },
          {
            "schema": {
              "type": "string",
              "example": "application/json"
            },
            "in": "header",
            "name": "Content-Type",
            "description": "application/json"
          }
        ],
        "requestBody": {
          "content": {
            "application/json": {
              "schema": {
                "$ref": "#/components/schemas/NerRequest"
              }
            }
          }
        },
        "x-stoplight": {
          "id": "0s1km6w8ct4u5"
        }
      },
      "parameters": [
        {
          "schema": {
            "type": "string",
            "example": "6a092bd4-5098-466c-94aa-40bf6829430\""
          },
          "name": "MODEL_ID",
          "in": "path",
          "required": true,
          "description": "Unique identifier for the model."
        }
      ]
    },
    "/ai/async-prediction/{PREDICTION_ID}": {
      "parameters": [
        {
          "schema": {
            "type": "string"
          },
          "name": "PREDICTION_ID",
          "in": "path",
          "required": true
        }
      ],
      "get": {
        "summary": "Get results by predictionId",
        "tags": [],
        "responses": {
          "200": {
            "description": "OK",
            "content": {
              "application/json": {
                "schema": {
                  "oneOf": [
                    {
                      "$ref": "#/components/schemas/PassthroughResponse"
                    },
                    {
                      "$ref": "#/components/schemas/RagExtDocResponse"
                    },
                    {
                      "$ref": "#/components/schemas/StandaloneResponse"
                    },
                    {
                      "$ref": "#/components/schemas/SummarizationResponse"
                    },
                    {
                      "$ref": "#/components/schemas/KeywordExtractionResponse"
                    },
                    {
                      "$ref": "#/components/schemas/NerResponse"
                    }
                  ]
                }
              }
            }
          },
          "4XX": {
            "$ref": "#/components/responses/Error"
          }
        },
        "operationId": "get-ai-async-prediction-predictionId",
        "x-stoplight": {
          "id": "55gaaapz3hmhm"
        }
      }
    }
  },
  "components": {
    "schemas": {
      "BasicGenericRequest": {
        "title": "BasicGenericRequest",
        "x-stoplight": {
          "id": "2dx9sqiottkts"
        },
        "type": "object",
        "description": "",
        "x-examples": {},
        "properties": {
          "batch": {
            "type": "array",
            "description": "The batch of key:value pairs used as inputs in the prediction. Up to 32 inputs per request are allowed.",
            "maxItems": 32,
            "items": {
              "type": "object",
              "properties": {
                "text": {
                  "type": "string",
                  "description": "The content the model analyzes.",
                  "example": "The content the model analyzes."
                }
              }
            }
          },
          "modelConfig": {
            "$ref": "#/components/schemas/ModelConfig"
          }
        }
      },
      "BatchRag": {
        "title": "BatchRag",
        "x-stoplight": {
          "id": "3ewd5zlcro9fh"
        },
        "type": "array",
        "description": "",
        "x-examples": {},
        "items": {
          "x-stoplight": {
            "id": "6ij3h4t6sz1ks"
          },
          "type": "object",
          "properties": {
            "text": {
              "type": "string",
              "x-stoplight": {
                "id": "8bzt3m6hmlo1d"
              },
              "description": "Content for the model to analyze. Multiple instances of text can be sent in the request.",
              "example": "What is RAG?"
            },
            "documents": {
              "type": "array",
              "items": {
                "$ref": "#/components/schemas/Document"
              }
            }
          }
        }
      },
      "Document": {
        "title": "Document",
        "type": "object",
        "description": "The document information the model analyzes.",
        "properties": {
          "body": {
            "type": "string",
            "x-stoplight": {
              "id": "pb4pg1j4sdyg5"
            },
            "description": "The contents of the document.",
            "example": "Retrieval Augmented Generation, known as RAG, a framework promising to optimize generative AI."
          },
          "source": {
            "type": "string",
            "x-stoplight": {
              "id": "twnscc6lc2z3z"
            },
            "description": "The URL that identifies the source of the document.",
            "example": "http://rag.com/22"
          },
          "title": {
            "type": "string",
            "x-stoplight": {
              "id": "7i8qpmfd3t49d"
            },
            "description": "The title of the document.",
            "example": "What are the benefits of RAG?"
          },
          "date": {
            "type": "string",
            "x-stoplight": {
              "id": "qbbwd5m0zq1vp"
            },
            "format": "date-time",
            "example": "2022-01-31T19:31:34Z",
            "description": "The date and time the document was created, displayed in the required ISO-8601 format of `yyyy-mm-ddThh:mm:ssZ`."
          }
        },
        "x-stoplight": {
          "id": "bkr03gtvoawvk"
        }
      },
      "JsonNode": {
        "type": "array",
        "description": "The list of model predictions for the input batch.",
        "example": [
          -0.0011799398344010115,
          0.07051781564950943,
          -0.06832550466060638,
          0.020428132265806198,
          0.11977626383304596
        ],
        "items": {},
        "x-stoplight": {
          "id": "w6zrspbzq8se1"
        }
      },
      "KeywordExtractionRequest": {
        "title": "KeywordExtractionRequest",
        "x-stoplight": {
          "id": "gnfyvpt0xzkyn"
        },
        "type": "object",
        "x-examples": {},
        "properties": {
          "batch": {
            "type": "array",
            "description": "The batch of key:value pairs used as inputs in the prediction. Up to 32 inputs per request are allowed.",
            "maxItems": 32,
            "items": {
              "type": "object",
              "properties": {
                "text": {
                  "type": "string",
                  "description": "The content the model analyzes.",
                  "example": "Joseph Robinette Biden Jr.is an American politician who is the 46th and current president of the United States. Ideologically a moderate member of the Democratic Party, he previously served as the 47th vice president from 2009 to 2017 under President Barack Obama and represented Delaware in the United States Senate from 1973 to 2009.Born in Scranton, Pennsylvania, Biden moved with his family to Delaware in 1953. He studied at the University of Delaware before earning his law degree from Syracuse University. He was elected to the New Castle County Council in 1970 and to the U.S. Senate in 1972. As a senator, Biden drafted and led the effort to pass the Violent Crime Control and Law Enforcement Act and the Violence Against Women Act. He also oversaw six U.S. Supreme Court confirmation hearings, including the contentious hearings for Robert Bork and Clarence Thomas. Biden ran unsuccessfully for the Democratic presidential nomination in 1988 and 2008. In 2008, Obama chose Biden as his running mate, and Biden was a close counselor to Obama during his two terms as vice president. In the 2020 presidential election, Biden and his running mate, Kamala Harris, defeated incumbents Donald Trump and Mike Pence. Biden is the second Catholic president in U.S. history (after John F. Kennedy), and his politics have been widely described as profoundly influenced by Catholic social teaching."
                }
              }
            }
          },
          "useCaseConfig": {
            "$ref": "#/components/schemas/UseCaseConfigKeywordExtraction"
          },
          "modelConfig": {
            "$ref": "#/components/schemas/ModelConfig"
          }
        }
      },
      "KeywordExtractionResponse": {
        "type": "object",
        "x-stoplight": {
          "id": "50i820dhx4lcl"
        },
        "x-examples": {},
        "title": "Response: Keyword extraction",
        "properties": {
          "predictionId": {
            "$ref": "#/components/schemas/predictionId"
          },
          "status": {
            "$ref": "#/components/schemas/status"
          },
          "predictions": {
            "type": "array",
            "items": {
              "$ref": "#/components/schemas/KeywordExtractionResponseTokens"
            }
          }
        }
      },
      "KeywordExtractionResponseTokens": {
        "type": "object",
        "x-stoplight": {
          "id": "xm8zls0dgmm2r"
        },
        "x-examples": {},
        "properties": {
          "response": {
            "type": "string",
            "description": "The results returned from the request.",
            "example": "Joseph Robinette Biden Jr., 46th president of the United States, Democratic Party, Vice President, Barack Obama, Delaware, University of Delaware, Syracuse University, Violent Crime Control and Law Enforcement Act, Violence Against Women Act."
          },
          "tokensUsed": {
            "$ref": "#/components/schemas/Token"
          },
          "keywords": {
            "type": "array",
            "description": "Keywords derived from the response. The number of keywords is limited to the `maxKeywords` field in the request. \n```json\n\"keywords\": [\n        \"Joseph Robinette Biden Jr.\",\n        \"46th president of the United States\",\n        \"Democratic Party\",\n        \"Vice President\",\n        \"Barack Obama\",\n        \"Delaware\",\n        \"University of Delaware\",\n        \"Syracuse University\",\n        \"Violent Crime Control and Law Enforcement Act\",\n        \"Violence Against Women Act\"\n      ]\n```",
            "items": {
              "type": "string",
              "example": "Joseph Robinette Biden Jr."
            }
          }
        }
      },
      "NerRequest": {
        "title": "NerRequest",
        "x-stoplight": {
          "id": "16ozcq1jkgn4c"
        },
        "type": "object",
        "x-examples": {},
        "properties": {
          "batch": {
            "type": "array",
            "description": "The batch of key:value pairs used as inputs in the prediction. Up to 32 inputs per request are allowed.",
            "maxItems": 32,
            "items": {
              "type": "object",
              "properties": {
                "text": {
                  "type": "string",
                  "description": "The content the model analyzes.",
                  "example": "Mahatma Gandhi, born on October 2, 1869, in Porbandar, India, led a life that profoundly shaped the course of history. Inspired by his principles of non-violence, truth, and civil disobedience, Gandhi became a pivotal figure in India'\\''s struggle for independence from British rule. His journey began as a lawyer in South Africa, where he experienced racial discrimination and injustice, sparking his commitment to social justice. Returning to India, he became the face of the nonviolent resistance movement, employing methods like peaceful protests, fasting, and marches. The iconic Salt March of 1930 exemplified his philosophy as thousands followed him in the defiance of salt taxes imposed by the British. Gandhi'\\''s ascetic lifestyle, clad in simple attire and practicing self-sufficiency, endeared him to the masses. Despite facing imprisonment multiple times, he remained steadfast in his pursuit of India'\\''s freedom. Tragically, Gandhi was assassinated on January 30, 1948, but his legacy endures globally as a symbol of peace, tolerance, and the transformative power of nonviolent resistance."
                }
              }
            }
          },
          "useCaseConfig": {
            "$ref": "#/components/schemas/UseCaseConfigNer"
          },
          "modelConfig": {
            "$ref": "#/components/schemas/ModelConfig"
          }
        }
      },
      "NerResponse": {
        "type": "object",
        "x-stoplight": {
          "id": "q7r8avob0ipy3"
        },
        "x-examples": {},
        "title": "Response: NER (Named Entity Recognition)",
        "properties": {
          "predictionId": {
            "$ref": "#/components/schemas/predictionId"
          },
          "status": {
            "$ref": "#/components/schemas/status"
          },
          "predictions": {
            "type": "array",
            "items": {
              "$ref": "#/components/schemas/NerResponseTokens"
            }
          }
        }
      },
      "NerResponseTokens": {
        "type": "object",
        "x-stoplight": {
          "id": "wu5v2ry4eobtd"
        },
        "x-examples": {},
        "properties": {
          "tokensUsed": {
            "$ref": "#/components/schemas/Token"
          },
          "entities": {
            "type": "object",
            "x-stoplight": {
              "id": "h06t4l9r3qp5e"
            },
            "properties": {
              "entity": {
                "type": "array",
                "x-stoplight": {
                  "id": "kppy5f6besh1h"
                },
                "description": "The result of the requested entity type that was a key with a list of example values to search.\nFor example:\n\n{\n\t\t\t\t\"Location\": [\n\t\t\t\t\t\"Porbandar\",\n\t\t\t\t\t\"India\",\n\t\t\t\t\t\"South Africa\"\n\t\t\t\t]\n\t\t\t}",
                "items": {
                  "x-stoplight": {
                    "id": "4n4922wbsof2f"
                  },
                  "type": "string"
                }
              }
            }
          },
          "response": {
            "type": "string",
            "description": "The results returned from the request.",
            "example": "{\\n\\\"Location\\\": [\\n\\\"Porbandar\\\",\\n\\\"India\\\"\\n]\\n}"
          }
        }
      },
      "ModelConfig": {
        "title": "ModelConfig",
        "x-stoplight": {
          "id": "6dvs9gawblqq4"
        },
        "type": "object",
        "description": "Provides fields and values that specify ranges for tokens. Fields used for specific use cases and models are specified. The default values are used if other values are not specified.",
        "properties": {
          "temperature": {
            "type": "number",
            "format": "float",
            "example": 0.8,
            "minimum": 0,
            "maximum": 2,
            "description": "A floating point number, sampling temperature between 0 and 2. A higher sampling temperature such as 0.8, results in more random output. A lower value such as 0.2 results in more focused output."
          },
          "topP": {
            "type": "number",
            "format": "float",
            "example": 1,
            "minimum": 1,
            "maximum": 1,
            "description": "A floating-point number that controls the cumulative probability of the top tokens to consider. Required range is [0, 1]. Set topP to 1 to consider all tokens."
          },
          "presencePenalty": {
            "type": "number",
            "format": "float",
            "minimum": -2,
            "maximum": 2,
            "description": "A floating-point number that penalizes new tokens based on whether they have already appeared in the text. Required range is [-2, 2]. A value greater than zero (0) encourages the model to use new tokens. A value less than zero (0) encourages the model to repeat existing tokens.",
            "example": 2
          },
          "frequencyPenalty": {
            "type": "number",
            "format": "float",
            "minimum": -2,
            "maximum": 2,
            "example": 1,
            "description": "A floating-point number that penalizes new tokens based on their frequency in the generated text. Required range is [-2, 2]. A value greater than zero (0) encourages the model to use new tokens. A value less than zero (0) encourages the model to repeat existing tokens."
          },
          "maxTokens": {
            "type": "integer",
            "format": "int32",
            "example": 1,
            "description": "The maximum number of tokens to generate per output sequence."
          },
          "apiKey": {
            "type": "string",
            "description": "This parameter is optional, and is only required when the specified model is used for prediction:\n\nThe syntax is:\n\n`\"apiKey\": \"[OPENAI_API_KEY]\"` for OpenAI models.\n\n`\"apiKey\": \"[KEY1] or [KEY2]\"` for the deployed Azure OpenAI model you want to use. Either key is a valid value in the request.\n\n`\"apiKey\": \"[BASE64_ENCODED_GOOGLE_SERVICE_ACCOUNT_KEY]\"` for Google VertexAI models.",
            "example": "API key specific to use case and model"
          },
          "azureDeployment": {
            "type": "string",
            "x-stoplight": {
              "id": "k7om8x7fmdk2a"
            },
            "description": "This optional parameter is the name of the deployed Azure OpenAI model and is only required when a deployed Azure OpenAI model is used for prediction.",
            "example": "DEPLOYMENT_NAME"
          },
          "azureEndpoint": {
            "type": "string",
            "x-stoplight": {
              "id": "igdfjnlqtdsto"
            },
            "description": "\nThis optional parameter is the URL endpoint of the deployed Azure OpenAI model and is only required when a deployed Azure OpenAI model is used for prediction.",
            "example": "https://azure.endpoint.com"
          },
          "googleProjectId": {
            "type": "string",
            "x-stoplight": {
              "id": "xwfd8sdgfquek"
            },
            "example": "[GOOGLE_PROJECT_ID]",
            "description": "This parameter is optional, and is only required when a Google VertexAI model is used for prediction.  "
          },
          "googleRegion": {
            "type": "string",
            "x-stoplight": {
              "id": "ev3hgv72zg8v1"
            },
            "description": "This parameter is optional, and is only required when a Google VertexAI model is used for prediction.  The possible region values are:\n\n* us-central1\n* us-west4\n* northamerica-northeast1\n* us-east4\n* us-west1\n* asia-northeast3\n* asia-southeast1\n* asia-northeast",
            "example": "[GOOGLE_PROJECT_REGION_OF_MODEL_ACCESS]"
          }
        }
      },
      "PassthroughRequest": {
        "title": "PassthroughRequest",
        "x-stoplight": {
          "id": "u9ogmu5gwx88z"
        },
        "type": "object",
        "x-examples": {},
        "properties": {
          "batch": {
            "type": "array",
            "description": "The batch of key:value pairs used as inputs in the prediction. Up to 32 inputs per request are allowed.",
            "maxItems": 32,
            "items": {
              "type": "object",
              "properties": {
                "text": {
                  "type": "string",
                  "x-stoplight": {
                    "id": "t59at9anv3pt7"
                  },
                  "description": "The content the model analyzes.",
                  "example": "[{\\\"role\\\": \\\"system\\\", \\\"content\\\": \\\"You are a helpful utility program instructed to accomplish a word correction task. Provide the most likely suggestion to the user without an preamble or elaboration.\\\"}, {\\\"role\\\": \\\"user\\\", \\\"content\\\": \\\"misspeled\\\"}, {\\\"role\\\": \\\"assistant\\\", \\\"content\\\": \\\"CORRECT:\\\"}]"
                }
              }
            }
          },
          "useCaseConfig": {
            "$ref": "#/components/schemas/UseCaseConfigPassthrough"
          },
          "modelConfig": {
            "$ref": "#/components/schemas/ModelConfig"
          }
        }
      },
      "PassthroughResponse": {
        "type": "object",
        "x-stoplight": {
          "id": "1aybbpg2d8uzr"
        },
        "x-examples": {},
        "title": "Response: Passthrough",
        "properties": {
          "predictionId": {
            "$ref": "#/components/schemas/predictionId"
          },
          "status": {
            "$ref": "#/components/schemas/status"
          },
          "predictions": {
            "type": "array",
            "items": {
              "$ref": "#/components/schemas/PassthroughResponseTokens"
            }
          }
        }
      },
      "PassthroughResponseTokens": {
        "type": "object",
        "x-stoplight": {
          "id": "v1hv9q4qf2lsm"
        },
        "properties": {
          "response": {
            "type": "string",
            "x-stoplight": {
              "id": "9ddycxisvubki"
            },
            "description": "The results returned from the request.",
            "example": "CORRECT: misspelled"
          },
          "tokensUsed": {
            "$ref": "#/components/schemas/Token"
          }
        }
      },
      "RagExtDocRequest": {
        "title": "RAG Request",
        "x-stoplight": {
          "id": "h308ctw14c79g"
        },
        "type": "object",
        "properties": {
          "batch": {
            "$ref": "#/components/schemas/BatchRag"
          },
          "useCaseConfig": {
            "$ref": "#/components/schemas/UseCaseConfigRagExtDoc"
          },
          "modelConfig": {
            "$ref": "#/components/schemas/ModelConfig"
          }
        }
      },
      "RagExtDocResponse": {
        "type": "object",
        "x-stoplight": {
          "id": "6y5uyvmeql5ty"
        },
        "x-examples": {},
        "title": "Response: RAG use case",
        "properties": {
          "predictionId": {
            "$ref": "#/components/schemas/predictionId"
          },
          "status": {
            "$ref": "#/components/schemas/status"
          },
          "predictions": {
            "type": "array",
            "items": {
              "$ref": "#/components/schemas/RagExtDocResponseTokens"
            }
          }
        },
        "description": ""
      },
      "RagExtDocResponseTokens": {
        "type": "object",
        "x-stoplight": {
          "id": "g45jjzwtkkd1e"
        },
        "properties": {
          "response": {
            "type": "string",
            "description": "The unparsed response returned from the request.",
            "example": "ANSWER: \\\"Retrieval Augmented Generation, known as RAG, a framework promising to optimize generative AI.\\\"\\nSOURCES: [\\\"http://example.com/112\\\"]"
          },
          "tokensUsed": {
            "$ref": "#/components/schemas/Token"
          },
          "answer": {
            "type": "string",
            "x-stoplight": {
              "id": "v6sanah1h9duk"
            },
            "description": "The parsed response text from the document. ",
            "example": "Retrieval Augmented Generation, known as RAG, a framework promising to optimize generative AI."
          },
          "sources": {
            "type": "string",
            "x-stoplight": {
              "id": "b20fctme42z6s"
            },
            "description": "The URL that identifies the source of the document returned in the response. Multiple results may be returned.",
            "example": "http://example.com/112"
          },
          "memoryUuid": {
            "type": "string",
            "x-stoplight": {
              "id": "m7m4pmps744ea"
            },
            "description": "The universal unique identifier (UUID) stored in the trained set of data in the model that is used in the model request.\n\nThis parameter is optional, and is used when previous chat history reference information is available.",
            "example": "27a887fe-3d7c-4ef0-9597-e2dfc054c20e"
          }
        }
      },
      "StandaloneQueryRewriterRequest": {
        "title": "StandaloneQueryRewriterRequest",
        "x-stoplight": {
          "id": "xkmy6mirw6k0g"
        },
        "type": "object",
        "x-examples": {},
        "properties": {
          "batch": {
            "type": "array",
            "description": "The batch of key:value pairs used as inputs in the prediction. Up to 32 inputs per request are allowed.",
            "maxItems": 32,
            "items": {
              "type": "object",
              "properties": {
                "text": {
                  "type": "string",
                  "description": "The content the model analyzes.",
                  "example": "Is it a framework?"
                }
              }
            }
          },
          "useCaseConfig": {
            "$ref": "#/components/schemas/UseCaseConfigStandalone"
          },
          "modelConfig": {
            "$ref": "#/components/schemas/ModelConfig"
          }
        }
      },
      "StandaloneResponse": {
        "type": "object",
        "x-stoplight": {
          "id": "lk858s1awuj5r"
        },
        "x-examples": {},
        "title": "Response: Standalone query rewriter",
        "properties": {
          "predictionId": {
            "$ref": "#/components/schemas/predictionId"
          },
          "status": {
            "$ref": "#/components/schemas/status"
          },
          "predictions": {
            "type": "array",
            "items": {
              "$ref": "#/components/schemas/StandaloneResponseTokens"
            }
          }
        }
      },
      "StandaloneResponseTokens": {
        "type": "object",
        "x-stoplight": {
          "id": "2viniqtddbvd1"
        },
        "properties": {
          "response": {
            "type": "string",
            "description": "The results returned from the request.",
            "example": "Is RAG a framework?"
          },
          "tokensUsed": {
            "$ref": "#/components/schemas/Token"
          }
        }
      },
      "SummarizationRequest": {
        "title": "SummarizationRequest",
        "x-stoplight": {
          "id": "q6ho79v3mmz6s"
        },
        "type": "object",
        "x-examples": {},
        "properties": {
          "batch": {
            "type": "array",
            "description": "The batch of key:value pairs used as inputs in the prediction. Up to 32 inputs per request are allowed.",
            "maxItems": 32,
            "items": {
              "type": "object",
              "properties": {
                "text": {
                  "type": "string",
                  "description": "The content the model analyzes.",
                  "example": "Nearly ten years had passed since the Dursleys had woken up to find their nephew on the front step, but Privet Drive had hardly changed at all. The sun rose on the same tidy front gardens and lit up the brass number four on the Dursleys''' front door; it crept into their living room, which was almost exactly the same as it had been on the night when Mr. Dursley had seen that fateful news report about the owls. Only the photographs on the mantelpiece really showed how much time had passed. Ten years ago, there had been lots of pictures of what looked like a large pink beach ball wearing different-colored bonnets - but Dudley Dursley was no longer a baby, and now the photographs showed a large blond boy riding his first bicycle, on a carousel at the fair, playing a computer game with his father, being hugged and kissed by his mother. The room held no sign at all that another boy lived in the house, too."
                }
              }
            }
          },
          "useCaseConfig": {
            "$ref": "#/components/schemas/UseCaseConfigSummarization"
          },
          "modelConfig": {
            "$ref": "#/components/schemas/ModelConfig"
          }
        }
      },
      "SummarizationResponse": {
        "type": "object",
        "x-stoplight": {
          "id": "t58p30n714n34"
        },
        "x-examples": {},
        "title": "Response: Summarization",
        "properties": {
          "predictionId": {
            "$ref": "#/components/schemas/predictionId"
          },
          "status": {
            "$ref": "#/components/schemas/status"
          },
          "predictions": {
            "type": "array",
            "items": {
              "$ref": "#/components/schemas/SummarizationResponseTokens"
            }
          }
        }
      },
      "SummarizationResponseTokens": {
        "type": "object",
        "x-stoplight": {
          "id": "ie77v9yhqcjxc"
        },
        "properties": {
          "tokensUsed": {
            "$ref": "#/components/schemas/Token"
          },
          "response": {
            "type": "string",
            "description": "The results returned from the request.",
            "example": "A decade after the Dursleys found their nephew on their doorstep, Privet Drive remains largely unchanged. The only noticeable difference is in the family photographs, which now depict Dudley Dursley as a large blond boy engaged in various activities, replacing his baby pictures. There is no indication of another boy living in the house."
          }
        },
        "x-examples": {}
      },
      "Token": {
        "type": "object",
        "properties": {
          "promptTokens": {
            "type": "integer",
            "format": "int32",
            "description": "The number of tokens generated to prompt the model to continue generating results.",
            "example": 148
          },
          "completionTokens": {
            "type": "integer",
            "format": "int32",
            "description": "The number of tokens used until the model completes.",
            "example": 27
          },
          "totalTokens": {
            "type": "integer",
            "format": "int32",
            "description": "The sum of the prompt and completion tokens used in the model.",
            "example": 175
          }
        },
        "x-stoplight": {
          "id": "sfjfh8yzyjzjr"
        }
      },
      "UseCaseConfigKeywordExtraction": {
        "title": "UseCaseConfigKeywordExtraction",
        "x-stoplight": {
          "id": "ypfxmkgddq9ps"
        },
        "type": "object",
        "properties": {
          "maxKeywords": {
            "type": "integer",
            "example": 100,
            "description": "This parameter specifies the maximum number of keywords that can be extracted.",
            "x-stoplight": {
              "id": "is5eeomo2utbw"
            }
          }
        }
      },
      "UseCaseConfigNer": {
        "title": "UseCaseConfigNer",
        "x-stoplight": {
          "id": "41rtokh4qfcps"
        },
        "type": "object",
        "properties": {
          "entityTypeMap": {
            "type": "object",
            "x-stoplight": {
              "id": "si2smqfu723we"
            },
            "properties": {
              "entity": {
                "type": "array",
                "x-stoplight": {
                  "id": "bn9xvied3h7zz"
                },
                "description": "\"entity\": [exampleA, exampleB], \"entity1\": [exampleC, exampleD]\n\nFor example:\n\n\"Location\": [\"India\", \"South Africa\"]\n\nThis parameter provides a map with entity type as a key with a list of example values to search. The entity type is required, but example values are optional and can be empty. Multiple entities with examples can be entered in the request.\n\nIn the Fusion Call Lucidworks AI Index stage] and the Fusion Call Lucidworks AI Query stage, the `useCaseConfig entityTypeMap` parameter only supports a string. Therefore, the string entered in Fusion is converted to a JSON string, which is required in the Lucidworks AI `entityTypeMap` variable. ",
                "items": {
                  "x-stoplight": {
                    "id": "fxa5v72vba7e1"
                  },
                  "type": "string",
                  "example": "example1, example2"
                }
              }
            }
          }
        }
      },
      "UseCaseConfigPassthrough": {
        "title": "UseCaseConfigPassthrough",
        "x-stoplight": {
          "id": "sr25b7exog1bh"
        },
        "type": "object",
        "properties": {
          "useSystemPrompt": {
            "type": "boolean",
            "description": "This optional parameter contains a default value of true. If set to false, the `batch.text` value serves as the prompt for the model. The prompt must be in a specific format the model can comprehend."
          },
          "dataType": {
            "type": "string",
            "x-stoplight": {
              "id": "5iw0tyg96y6g9"
            },
            "description": "This optional parameter enables model-specific handling in the Prediction API to help improve model accuracy. Use the most applicable fields based on available dataTypes and the dataType value that best aligns with the text sent to the Prediction API.\n\nThe values for `dataType` in the Passthrough use case are:\n\n* `\"dataType\": \"text\"` - This value is equivalent to `\"useSystemPrompt\": true` and is a pre-defined, generic prompt.\n\n* `\"dataType\": \"raw_prompt\"` - This value is equivalent to `\"useSystemPrompt\": false` and is passed directly to the model or third-party API.\n\n* `\"dataType\": \"json_prompt\"` - This value follows the generics that allow three roles:\n    - `system`\n\n    - `user` \n        - Only the last user message is truncated.\n        - If the API does not support system prompts, the user role is substituted for the system role.\n\n    - `assistant`\n        - If the last message role is `assistant`, it is used as a pre-fill for generation and is the first generated token the model uses. The pre-fill is prepended to the model output, which makes models less verbose and helps enforce specific outputs such as YAML.\n        - The Google VertexAI does not support generation pre-fills, so an exception error is generated.\n\n    - This follows the HuggingFace template contraints at https://huggingface.co/docs/transformers/main/en/chat_templating.\n\n    - Additional `json_prompt` information:\n\n        - Consecutive messages for the same role are merged.\n        - You can paste the information for a hosted model into the `json_prompt` value and change the model name in the stage. ",
            "example": "text"
          }
        }
      },
      "UseCaseConfigRagExtDoc": {
        "title": "UseCaseConfigRagChat",
        "x-stoplight": {
          "id": "qp5eq4847udi8"
        },
        "type": "object",
        "properties": {
          "memoryUuid": {
            "type": "string",
            "description": "The universal unique identifier (UUID) stored in the trained set of data in the model that is used in the model request.\n\nThis parameter is optional, and is used when previous chat history reference information is available.",
            "example": "27a887fe-3d7c-4ef0-9597-e2dfc054c20e"
          },
          "extractRelevantContent": {
            "type": "boolean",
            "description": "This determines if relevant content can be extracted in the request.",
            "default": false
          }
        }
      },
      "UseCaseConfigStandalone": {
        "title": "UseCaseConfigStandalone",
        "x-stoplight": {
          "id": "ra60bh957z7j9"
        },
        "type": "object",
        "properties": {
          "memoryUuid": {
            "type": "string",
            "description": "The universal unique identifier (UUID) stored in the trained set of data in the model that is used in the model request.\n\nThis parameter is optional, and is used when previous chat history reference information is available.",
            "example": "27a887fe-3d7c-4ef0-9597-e2dfc054c20e"
          }
        }
      },
      "UseCaseConfigSummarization": {
        "title": "UseCaseConfigSummarization",
        "x-stoplight": {
          "id": "g28n8g40ilnep"
        },
        "type": "object",
        "properties": {
          "maxWords": {
            "type": "integer",
            "example": 100,
            "description": "This parameter specifies the maximum number of words returned in the summary when generated by the model."
          }
        }
      },
      "predictionId": {
        "title": "predictionId",
        "x-stoplight": {
          "id": "ldm419de0e7o7"
        },
        "type": "string",
        "description": "The universal unique identifier (UUID) returned in the POST request. This UUID is required in the GET request to retrieve results.",
        "format": "uuid",
        "example": "fd110486-f168-47c0-a419-1518a4840589"
      },
      "status": {
        "title": "status",
        "x-stoplight": {
          "id": "zqgjt7snebql4"
        },
        "type": "string",
        "description": "The current status of the prediction. Allowed values are:\n\n* SUBMITTED - The POST request was successful and the response has returned the `predictionId` and `status` that is used by the GET request.\n\n* READY - The results associated with the `predictionId` are available and ready to be retrieved.\n\n* ERROR - An error was generated when the GET request was sent.\n\n* RETRIEVED - The results associated with the `predicitonId` are returned successfully when the GET request was sent.",
        "example": "READY"
      },
      "POSTresponse": {
        "title": "POST response",
        "x-stoplight": {
          "id": "syk5kziwrvbls"
        },
        "type": "object",
        "description": "This is the response to the POST prediction request submitted for a specific `useCase` and `modelId`. ",
        "properties": {
          "predictionId": {
            "type": "string",
            "format": "uuid",
            "description": "The universal unique identifier (UUID) returned in the POST request. This UUID is required in the GET request to retrieve results."
          },
          "status": {
            "type": "string",
            "example": "SUBMITTED",
            "description": "The current status of the prediction. Allowed values are:\n\n* SUBMITTED - The POST request was successful and the response has returned the `predictionId` and `status` that is used by the GET request.\n\n* ERROR - An error was generated when the GET request was sent.\n\n* READY - The results associated with the `predictionId` are available and ready to be retrieved.\n\n* RETRIEVED - The results associated with the `predicitonId` are returned successfully when the GET request was sent."
          }
        }
      }
    },
    "securitySchemes": {
      "apiKey_1": {
        "name": "apiKey",
        "type": "apiKey",
        "in": "query"
      }
    },
    "responses": {
      "Error": {
        "description": "The error varies based on the issue encountered regarding the `predictionId` or related information.",
        "content": {
          "application/json": {
            "schema": {
              "type": "object",
              "properties": {
                "predictionId": {
                  "$ref": "#/components/schemas/predictionId"
                },
                "status": {
                  "$ref": "#/components/schemas/status"
                },
                "message": {
                  "type": "string",
                  "x-stoplight": {
                    "id": "ord78ao578txs"
                  },
                  "description": "The error generated if the `predictionId` cannot be located or the related information cannot be retrieved. For example, \"System prompt exceeded the maximum number of allowed input tokens.\""
                }
              }
            }
          }
        }
      }
    }
  }
}